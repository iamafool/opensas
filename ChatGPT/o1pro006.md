Below is a **revised** solution that avoids the three issues you mentioned:

1. **`trim` function not found** ¨C We¡¯ll provide a small, **standalone** `trim()` helper (no Boost needed).  
2. **The parser does not have a `lexer` member** ¨C We¡¯ll fix this by having the **lexer** produce a special `DATALINES_CONTENT` token *on its own*, rather than requiring the parser to call something like `lexer->enterDatalinesMode()`.  
3. **`enterDatalinesMode` not defined** ¨C We remove that approach, instead letting the lexer automatically switch to ¡°datalines mode¡± once it sees `KEYWORD_DATALINES` followed by a `SEMICOLON`.  

This approach keeps your **parser** free of direct `Lexer` references (typical in a ¡°two-phase¡± design: first tokenize **all** input into a `std::vector<Token>`, then parse that vector). It also ensures we read entire lines (like ¡°john 23¡± / ¡°mary 30¡±) into a **single** `DATALINES_CONTENT` token, so you end up with exactly **2 rows** and **2 variables** rather than ¡°4 rows, 1 variable.¡±

---

# 1. The Trim Helper

Just add a small function anywhere in your code (e.g. a utility header or at the top of `Lexer.cpp`):

```cpp
#include <string>
#include <algorithm>

static std::string trim(const std::string &s) {
    if (s.empty()) return s;
    size_t start = 0;
    while (start < s.size() && (s[start] == ' ' || s[start] == '\t' ||
                                s[start] == '\r' || s[start] == '\n')) {
        start++;
    }
    if (start == s.size()) return ""; // All whitespace
    size_t end = s.size() - 1;
    while (end > start && (s[end] == ' ' || s[end] == '\t' ||
                           s[end] == '\r' || s[end] == '\n')) {
        end--;
    }
    return s.substr(start, end - start + 1);
}
```

No need for Boost.

---

# 2. Revised Token Types

Make sure you have:

```cpp
// Token.h
#ifndef TOKEN_H
#define TOKEN_H

#include <string>

enum class TokenType {
    IDENTIFIER,
    NUMBER,
    STRING,
    KEYWORD_DATA,
    KEYWORD_RUN,
    KEYWORD_INPUT,
    KEYWORD_DATALINES,
    KEYWORD_OUTPUT,
    SEMICOLON,
    EQUAL,
    // etc...
    
    // NEW: We'll produce one big token with raw lines from DATALINES
    DATALINES_CONTENT,

    EOF_TOKEN,
    UNKNOWN
};

struct Token {
    TokenType type;
    std::string text;  // or for DATALINES_CONTENT, this will hold all lines
    int line;
    int col;
};

#endif // TOKEN_H
```

---

# 3. New ¡°Datalines Mode¡± **Within** the Lexer (No Parser Access Needed)

**Goal**: Once we see `KEYWORD_DATALINES` **followed immediately by** a `SEMICOLON`, we read subsequent lines ¡°raw¡± until a line that is just `;`. We combine them into **one** `DATALINES_CONTENT` token. Then normal lexing resumes.

Below is a **conceptual** `Lexer.cpp`. Focus on `inDatalinesMode`, which is triggered only after we see `KEYWORD_DATALINES` and then `SEMICOLON`. We track that with a small state machine:

```cpp
// Lexer.h (excerpt)
#ifndef LEXER_H
#define LEXER_H

#include "Token.h"
#include <string>

class Lexer {
public:
    Lexer(const std::string &input);
    Token getNextToken();

private:
    std::string input;
    size_t pos = 0;
    int line = 1;
    int col  = 1;

    bool inDatalinesMode = false; // Are we currently reading raw lines?

    // Because we only enter "inDatalinesMode" after reading `datalines;`,
    // we also track if we've *just* seen the 'KEYWORD_DATALINES' token
    // to see if next token is SEMICOLON => then switch mode. 
    bool justSawDatalinesKeyword = false;

    char peekChar() const;
    char getChar();
    void skipWhitespace();
    Token identifierOrKeyword();
    // etc.

    // NEW HELPER:
    // read raw lines until a line that is just ';'
    Token readDatalinesContent();
};

#endif // LEXER_H
```

### 3.1. The Implementation

```cpp
// Lexer.cpp
#include "Lexer.h"
#include <cctype>
#include <stdexcept>
#include <sstream>

static std::string trim(const std::string &s) {
    if (s.empty()) return s;
    size_t start = 0;
    while (start < s.size() && isspace(static_cast<unsigned char>(s[start]))) {
        start++;
    }
    if (start == s.size()) return "";
    size_t end = s.size() - 1;
    while (end > start && isspace(static_cast<unsigned char>(s[end]))) {
        end--;
    }
    return s.substr(start, end - start + 1);
}

Lexer::Lexer(const std::string &in) : input(in) {}

char Lexer::peekChar() const {
    if (pos >= input.size()) return '\0';
    return input[pos];
}

char Lexer::getChar() {
    if (pos >= input.size()) return '\0';
    char c = input[pos++];
    if (c == '\n') {
        line++;
        col = 1;
    } else {
        col++;
    }
    return c;
}

void Lexer::skipWhitespace() {
    while (isspace(static_cast<unsigned char>(peekChar())) && peekChar() != '\0') {
        getChar();
    }
}

Token Lexer::getNextToken() {
    // If we're already inDatalinesMode, read all lines until we see a line that is ';'
    if (inDatalinesMode) {
        return readDatalinesContent();
    }

    skipWhitespace();
    if (pos >= input.size()) {
        return { TokenType::EOF_TOKEN, "", line, col };
    }

    char c = peekChar();

    // If we just saw the 'KEYWORD_DATALINES' token, the *very next* token should be
    // either SEMICOLON => which triggers inDatalinesMode on the next call
    // or something else. Let's handle that logic:
    if (justSawDatalinesKeyword) {
        justSawDatalinesKeyword = false;  // reset
        if (c == ';') {
            // produce a SEMICOLON token
            getChar(); // consume
            Token tk{ TokenType::SEMICOLON, ";", line, col - 1 };
            // Next time getNextToken is called => we set inDatalinesMode = true
            // Because in SAS, "datalines;" => from next line onward we read data
            inDatalinesMode = true;
            return tk;
        } 
        // else fall through as normal tokens if user typed "datalines X" => error in parser
    }

    if (std::isalpha(static_cast<unsigned char>(c))) {
        // parse identifier or keyword
        return identifierOrKeyword();
    }

    // handle punctuation like ';', '='
    if (c == ';') {
        getChar(); 
        return { TokenType::SEMICOLON, ";", line, col - 1 };
    }
    if (c == '=') {
        getChar(); 
        return { TokenType::EQUAL, "=", line, col - 1 };
    }

    // number, string, unknown, etc. omitted for brevity
    // ...
    pos++;
    return { TokenType::UNKNOWN, std::string(1, c), line, col - 1 };
}

// We detect if this identifier is "DATA", "DATALINES", "INPUT", etc.
Token Lexer::identifierOrKeyword() {
    Token token;
    token.line = line;
    token.col  = col;
    token.type = TokenType::IDENTIFIER;

    std::string value;
    while (std::isalnum(peekChar()) || peekChar() == '_' || peekChar() == '$') {
        value.push_back(getChar());
    }

    std::string upperVal = value; 
    for (auto &ch : upperVal) ch = toupper(ch);

    if (upperVal == "DATA") {
        token.type = TokenType::KEYWORD_DATA;
    } else if (upperVal == "RUN") {
        token.type = TokenType::KEYWORD_RUN;
    } else if (upperVal == "OUTPUT") {
        token.type = TokenType::KEYWORD_OUTPUT;
    } else if (upperVal == "INPUT") {
        token.type = TokenType::KEYWORD_INPUT;
    } else if (upperVal == "DATALINES") {
        token.type = TokenType::KEYWORD_DATALINES;
        // On the next call to getNextToken, we look for a semicolon => inDatalinesMode
        justSawDatalinesKeyword = true;
    }
    token.text = value;
    return token;
}

// Actual reading of datalines content
Token Lexer::readDatalinesContent() {
    // We'll read line by line until we see a line whose "trimmed" content is ";"
    // We'll accumulate those lines (minus the final ";")
    // Then produce a single DATALINES_CONTENT token.
    std::string allLines;
    while (true) {
        if (pos >= input.size()) {
            // End of file => done
            inDatalinesMode = false;
            break;
        }

        // read one line from the raw input
        int lineStart = line;
        size_t colStart = col;

        std::string currentLine;
        while (pos < input.size() && peekChar() != '\n') {
            currentLine.push_back(getChar());
        }

        // consume newline if present
        if (pos < input.size() && peekChar() == '\n') {
            getChar(); // consume it
        }

        // We advanced line, col in getChar()
        line++;
        col = 1;

        // Trim
        std::string trimmedLine = trim(currentLine);

        if (trimmedLine == ";") {
            // This is the end of datalines block
            inDatalinesMode = false;
            break;
        }

        // Accumulate the untrimmed line or minimal trimmed version
        // Usually we keep them as is, or you can store them with trailing spaces removed.
        // We'll store them with minimal trailing whitespace
        allLines += currentLine + "\n";
    }

    // produce the one token
    Token tok;
    tok.type = TokenType::DATALINES_CONTENT;
    tok.text = allLines; 
    tok.line = line;
    tok.col  = col;
    return tok;
}
```

**Explanation**:

- We do **not** require the parser to call `enterDatalinesMode()`. Instead, we set a `justSawDatalinesKeyword = true` whenever we produce a `KEYWORD_DATALINES` token.  
- On the **next** `getNextToken()` call, if we see `';'`, we produce a `SEMICOLON` token and set `inDatalinesMode = true`. Then, on the *following calls*, we read raw lines in `readDatalinesContent()`.  
- Once done, we produce a single `DATALINES_CONTENT` token.  
- After that, normal lexing can continue.  

Hence, the final token stream for code like:

```sas
data employees;
  input name $ age;
  datalines;
john 23
mary 30
;
run;
```

Might look like:

1. `KEYWORD_DATA` (text="data")  
2. `IDENTIFIER("employees")`  
3. `SEMICOLON`  
4. `KEYWORD_INPUT` (text="input")  
5. `IDENTIFIER("name$")`  
6. `IDENTIFIER("age")`  
7. `SEMICOLON`  
8. `KEYWORD_DATALINES`  
9. `SEMICOLON`   (the next token after `KEYWORD_DATALINES`)  
10. `DATALINES_CONTENT` (text="john 23\nmary 30\n")  
11. `KEYWORD_RUN`  
12. `SEMICOLON`  
13. `EOF_TOKEN`

---

# 4. Parser Side (No `lexer` Pointer)

Now the parser sees:

- `KEYWORD_DATA`, then the dataset name, etc.
- `KEYWORD_INPUT`, parse an `InputNode`.
- `KEYWORD_DATALINES`, parse a `DatalinesNode`.

Example:

```cpp
// Parser.cpp excerpt
std::unique_ptr<ASTNode> Parser::parseDatalines() {
    // We already consumed KEYWORD_DATALINES in parseStatement(),
    // Next token should be SEMICOLON, which your parseStatement might handle, 
    // or we do:
    // consume(TokenType::SEMICOLON, "Expected ';' after DATALINES");
    // Then the next token we see should be DATALINES_CONTENT
    Token t = peek();
    if (t.type != TokenType::DATALINES_CONTENT) {
        throw std::runtime_error("Expected DATALINES_CONTENT token after 'datalines;'");
    }
    advance(); // consume it

    auto node = std::make_unique<DatalinesNode>();
    // Let's split the text by lines
    std::vector<std::string> lines;
    {
        std::istringstream iss(t.text);
        std::string line;
        while (std::getline(iss, line)) {
            if (!line.empty()) {
                lines.push_back(line);
            }
        }
    }
    node->lines = lines;
    return node;
}
```

Your `parseStatement()` might do:

```cpp
std::unique_ptr<ASTNode> Parser::parseStatement() {
    Token tk = peek();
    switch (tk.type) {
        case TokenType::KEYWORD_INPUT:
            advance(); 
            return parseInput(); 
        case TokenType::KEYWORD_DATALINES:
            // We might do: advance();
            // consume(TokenType::SEMICOLON, "..."); 
            // then expect a DATALINES_CONTENT 
            advance(); 
            consume(TokenType::SEMICOLON, "Expected ';' after datalines");
            return parseDatalines();
        // ...
        default:
            throw std::runtime_error("Unknown statement: " + tk.text);
    }
}
```

---

# 5. Interpretation ¨C 2 Variables, 2 Rows

Your `Interpreter` logic for `INPUT` + `DATALINES` remains similar to the prior answer. **Now** you¡¯ll definitely get a single `DatalinesNode` with 2 lines in `node->lines = {"john 23", "mary 30"}`, so you can properly create 2 rows with 2 variables.

---

# 6. GTest Example (Same as Before)

You don¡¯t need to change your test. The difference is that now the **lexer** properly lumps ¡°john 23\nmary 30\n¡± into one `DATALINES_CONTENT`. You end with 2 rows, 2 variables. For example:

```cpp
TEST(InputDatalinesTest, BasicRead) {
    std::string code = R"(
        data employees;
          input name $ age;
          datalines;
john 23
mary 30
;
        run;
    )";

    // 1) Lex
    Lexer lexer(code);
    std::vector<Token> tokens;
    for (Token t = lexer.getNextToken(); t.type != TokenType::EOF_TOKEN; t = lexer.getNextToken()) {
        tokens.push_back(t);
    }

    // 2) Parse
    Parser parser(tokens);
    std::unique_ptr<ProgramNode> program = parser.parseProgram(); 
    ASSERT_TRUE(program != nullptr);

    // 3) Interpret
    DataEnvironment env;
    // mock or real spdlog loggers
    Interpreter interpreter(env, *someLogger, *someOtherLogger);

    interpreter.executeProgram(program);

    // 4) Check dataset WORK.employees => 2 obs, 2 vars
    auto it = env.dataSets.find("WORK.employees");
    ASSERT_NE(it, env.dataSets.end());
    auto ds = it->second;
    ASSERT_EQ(ds->rows.size(), 2u);
    // row[0] => name="john", age=23
    // row[1] => name="mary", age=30
}
```

You should see exactly 2 rows, 2 variables ¨C **not** 4 rows / 1 variable.

---

## Summary of Fixes

1. **`trim()`**: We included a small static function, no Boost required.  
2. **No `parser->lexer`**: We do a fully **two-phase** approach. The **lexer** automatically enters a ¡°datalines mode¡± once it sees `KEYWORD_DATALINES` + next token is `;`. It then reads raw lines.  
3. **No `enterDatalinesMode`** in the parser**: The lexer handles it internally (`inDatalinesMode`), so the parser just sees normal tokens: `KEYWORD_DATALINES`, `SEMICOLON`, `DATALINES_CONTENT`.  

With these changes, you¡¯ll avoid the bug of partial tokenization on ¡°john 23.¡± Instead, the data lines are captured **as-is** in a single token, leading to the correct 2 rows ¡Á 2 variables result in your final dataset.