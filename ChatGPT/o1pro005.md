Below is an **updated and more robust** approach for handling `DATALINES` (and its terminating semicolon) so that you reliably read **entire lines** until the ¡°standalone `;`¡± line, rather than inadvertently tokenizing them into multiple tokens. This solution is closer to how real SAS treats ¡°datalines¡± as a **raw data mode**. We¡¯ll outline:

1. **A special approach** for the **Lexer** (or Parser) to switch into a ¡°datalines mode¡± once `datalines;` is encountered.  
2. **Gather** the raw lines from the input, ignoring normal tokenization, until we see **a line containing only `;`**.  
3. **Produce** a single `DatalinesNode` that has all lines neatly stored.  
4. **Interpret** them, mapping them to variables defined by `INPUT`.

This avoids partial tokenization of the data lines, thus fixing the ¡°4 rows and 1 variable¡± bug.

---

# 1. High-Level Summary of the Fix

**Root cause**: If you rely on normal tokenization for lines after `datalines;`, the lexer sees each space-separated token (e.g., `john`, `23`, `mary`, `30`) as separate tokens. Then each might get interpreted as separate ¡°lines,¡± leading to extra rows or a single variable. **SAS** expects you to read full lines until a line containing only `;`.

**Better solution**: Once we see `datalines;`, we do:

1. **Immediately** ignore normal token logic.  
2. **Read entire lines** from the raw input, storing them in `DatalinesNode::lines`.  
3. Stop once we encounter a line that has only `;`.  
4. Resume normal lexing/parsing after that.

You can implement this in either:
- A **special Lexer** state (¡°DATALINES state¡±), or
- A **Parser** trick that grabs raw lines from the input buffer.

Below is a **lexer-based** approach, which is often simpler if you want your parser to remain straightforward (the parser doesn¡¯t need to parse tokens for the data lines).

---

# 2. Lexer Enhancement: ¡°Datalines Mode¡±

We¡¯ll show how to modify the **Lexer** such that:

- When you see `KEYWORD_DATALINES`, you **also** see a `SEMICOLON`.  
- Then the lexer **switches** to a ¡°datalines mode,¡± reading raw lines until it finds a line whose trimmed content is just `;`.

We can produce a **single token** like `TokenType::DATALINES_CONTENT` containing all lines as one string, or produce a dedicated `DATALINES` token that includes these lines in a structure. The parser can then create `DatalinesNode`.

Below is an example approach:

```cpp
// TokenType addition
enum class TokenType {
    // ...
    KEYWORD_DATALINES,
    DATALINES_CONTENT,  // a special token that holds all lines raw
    // ...
};

// In your Lexer class, add a flag or state:
class Lexer {
public:
    Lexer(const std::string &input);
    Token getNextToken();

private:
    std::string input;
    size_t pos = 0;
    int line = 1;
    int col = 1;

    bool inDatalinesMode = false;  // <-- We'll set this to true after we see 'datalines;'

    // Helper methods: skipWhitespace, peekChar, etc.
    // ...
};
```

### 2.1. Generating a `DATALINES_CONTENT` token

When we detect `datalines;` in normal mode, we do:

```cpp
Token Lexer::getNextToken() {
    skipWhitespace();

    // If not in datalines mode:
    if (!inDatalinesMode) {
        if (pos >= input.size()) {
            return { TokenType::EOF_TOKEN, "", line, col };
        }

        // Normal lexing logic for comments, strings, keywords
        // Suppose we see `KEYWORD_DATALINES`. Then we also see a `SEMICOLON`.

        // If we produce `KEYWORD_DATALINES` as normal, then the parser sees it and 
        // expects to see a semicolon next. Once the parser consumes that semicolon, 
        // it knows it¡¯s time to read raw lines. Alternatively, you can switch 
        // inDatalinesMode immediately once you see `KEYWORD_DATALINES`, 
        // but typically SAS expects `datalines;` as two tokens, so let¡¯s do it 
        // right after we produce `KEYWORD_DATALINES`.

        // Implementation details vary. 
        // For demonstration, let¡¯s say after we produce KEYWORD_DATALINES token,
        // the parser consumes it and the semicolon. Then the parser calls a method 
        // or the next getNextToken sees the inDatalinesMode = true. We'll show 
        // a different approach:

        // If we see DATALINES in identifierOrKeyword():
        //   if (upperValue == "DATALINES") {
        //       token.type = TokenType::KEYWORD_DATALINES;
        //   }

        // Then once the parser has consumed 'KEYWORD_DATALINES' and 'SEMICOLON', 
        // it can call a method on the lexer: setInDatalinesMode(true).

        // ...
    } 
    else {
        // If inDatalinesMode:
        // We'll read lines until we find a line with only ";"
        if (pos >= input.size()) {
            // End of file => end datalines
            inDatalinesMode = false; // or we can just keep it true but we're out of tokens
            return { TokenType::EOF_TOKEN, "", line, col };
        }

        // Collect lines in a big buffer or read them one line at a time 
        // until we find a line that is just ';'.
        std::string allLines;
        while (pos < input.size()) {
            // read a single line
            int startLine = line;
            size_t lineStartPos = pos;
            std::string thisLine;

            // read until newline or EOF
            while (pos < input.size() && input[pos] != '\n') {
                thisLine.push_back(input[pos++]);
                col++;
            }
            // consume the newline if present
            if (pos < input.size() && input[pos] == '\n') {
                pos++;
                line++;
                col = 1;
            }

            // trim the line
            auto trimmed = trim(thisLine);
            if (trimmed == ";") {
                // we found the termination line => stop reading data lines
                inDatalinesMode = false;
                break;
            }

            // accumulate thisLine in allLines with a newline
            allLines += thisLine + "\n";

            if (!inDatalinesMode) {
                // means we encountered the line with just ';'
                break;
            }
        }

        // Now produce a single token with type DATALINES_CONTENT
        Token tok;
        tok.type = TokenType::DATALINES_CONTENT;
        tok.text = allLines;
        tok.line = line;
        tok.col  = col;
        return tok;
    }

    // ... the rest of normal lex logic
    // ...
}
```

**Note**: This approach **absorbs** all lines after `datalines;` into **one** token (`DATALINES_CONTENT`). Once we return that token, the parser can build a `DatalinesNode`.

**Alternatively**, you can produce multiple tokens, one per line, but typically you just want a single chunk.

### 2.2. Telling the Lexer to Enter ¡°datalines mode¡±

One typical approach: 
- The **parser** calls a function on the lexer **once** it consumes `datalines;`. 
- Something like `lexer.enterDatalinesMode();` sets `inDatalinesMode=true`. 
- The next call to `getNextToken()` returns the single `DATALINES_CONTENT` token.

**Example**:

```cpp
// In Parser::parseDatalines()
std::unique_ptr<ASTNode> Parser::parseDatalines() {
    consume(TokenType::KEYWORD_DATALINES, "Expected 'datalines'");
    consume(TokenType::SEMICOLON, "Expected ';' after datalines");
    // now tell the lexer
    lexer->enterDatalinesMode();  // or set a flag

    // next token should be DATALINES_CONTENT
    Token t = lexer->getNextToken();
    if (t.type != TokenType::DATALINES_CONTENT) {
        throw std::runtime_error("Expected data lines after datalines; but got token: " + t.text);
    }

    // parse that into lines
    auto node = std::make_unique<DatalinesNode>();
    node->lines = splitIntoLines(t.text); // e.g. split by '\n'
    // then we expect the parser to continue normally after that
    return node;
}
```

Where `splitIntoLines()` is a small helper:

```cpp
std::vector<std::string> splitIntoLines(const std::string &text) {
    std::vector<std::string> lines;
    std::istringstream iss(text);
    std::string line;
    while (std::getline(iss, line)) {
        if (!line.empty()) {
            lines.push_back(line);
        }
    }
    return lines;
}
```

Hence, you get:

- `KEYWORD_DATALINES`
- `SEMICOLON`
- (Parser calls `lexer.enterDatalinesMode()`)
- Next token => `DATALINES_CONTENT` with the entire chunk.

That chunk is parted into lines for the `DatalinesNode`.

---

# 3. Handling 2 Rows / 2 Variables

With this robust approach:

1. If your code is:
   ```sas
   data employees;
     input name $ age;
     datalines;
john 23
mary 30
;
   run;
   ```
2. The parser sees tokens:  
   - `KEYWORD_DATA`, `IDENTIFIER("employees")`, `SEMICOLON`,  
   - `KEYWORD_INPUT`, `IDENTIFIER("name$")`, `IDENTIFIER("age")`, `SEMICOLON`,  
   - `KEYWORD_DATALINES`, `SEMICOLON`,  
   - Then the **`DATALINES_CONTENT`** token: (the text `"john 23\nmary 30\n"`).  
   - Then eventually `KEYWORD_RUN`, `SEMICOLON`.
3. The parser forms:
   - `DataStepNode(outputDataSet="employees")` with statements:  
     - `InputNode(variables=["name$", "age"])`  
     - `DatalinesNode(lines=["john 23", "mary 30"])`  
4. The interpreter uses that node, sees 2 input variables (`name$`, `age`), 2 lines => you get 2 rows, 2 variables.

**No** extra spurious tokens or lines.

---

# 4. Example GTest to Confirm 2 Rows, 2 Vars

We can keep the **same** GTest as in the previous example, but now your system actually produces 2 rows and 2 variables:

```cpp
TEST(InputDatalinesTest, BasicRead) {
    std::string code = R"(
        data employees;
          input name $ age;
          datalines;
john 23
mary 30
;
        run;
    )";

    // Lexer, Parser, Interpreter as usual
    Lexer lexer(code);
    std::vector<Token> tokens;
    for (Token tk = lexer.getNextToken(); tk.type != TokenType::EOF_TOKEN; tk = lexer.getNextToken()) {
        tokens.push_back(tk);
    }

    Parser parser(tokens);
    auto program = parser.parseProgram(); // or parseStatement, etc.

    DataEnvironment env;
    Interpreter interpreter(env, /* loggers */);

    interpreter.executeProgram(program);

    // Expect WORK.employees with 2 obs, 2 vars
    auto it = env.dataSets.find("WORK.employees");
    ASSERT_NE(it, env.dataSets.end());
    auto ds = it->second;
    ASSERT_EQ(ds->rows.size(), 2u);
    // first row => name="john", age=23
    // second row => name="mary", age=30
    // etc. (same checks as before)
}
```

You should see it produce exactly 2 rows, 2 variables.

---

# 5. Recap / Key Points

1. **Don¡¯t** tokenize lines after `datalines;` in the normal way. Real SAS effectively reads them as raw lines, ignoring token boundaries.  
2. **A ¡°datalines mode¡±** in the Lexer is typical. Once we see `datalines;`, we read line by line until a line with just `;`.  
3. We produce a single token that lumps all those lines together.  
4. The **Parser** builds a `DatalinesNode` from that content.  
5. The **Interpreter** uses `InputNode` variable definitions plus the lines from `DatalinesNode` to create the correct number of rows and columns.

With this approach, you should no longer get ¡°4 rows and 1 variable.¡± Instead, you¡¯ll properly parse 2 lines of data => 2 rows, with each line split into 2 fields => 2 variables.

**Good luck** integrating this more robust fix into your code base!